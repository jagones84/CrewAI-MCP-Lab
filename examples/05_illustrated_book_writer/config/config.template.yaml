# =============================================================================
# INFRASTRUCTURE & RESOURCE CONFIGURATION
# =============================================================================
# [IMPORTANT] THIS FILE MUST CONTAIN ALL POSSIBLE CONFIGURATIONS
# DO NOT REMOVE UNUSED PROFILES. USERS MUST BE ABLE TO SWITCH BY CHANGING 'llm_selected'
# =============================================================================
infrastructure:
  # SELECT ACTIVE PROVIDERS
  # Options: "openrouter", "ollama", "llama_cp_local", "vllm", "llama_cp", "llama_cp_tunnel"
  llm_selected: "ollama" 
  
  # Options: "remote_dgspark" (dgspark), "local_standard" (PC)
  image_selected: "local_standard" 
  
  llm_profiles:
    # 1. OpenRouter (Cloud)
    openrouter:
      model: "openai/gpt-4o-mini" # or "anthropic/claude-3-haiku"
      base_url: "https://openrouter.ai/api/v1"
      temperature: 0.7
      # api_key: "sk-or-..." # UNCOMMENT AND ADD YOUR KEY HERE
      
    # 2. Ollama (Local)
    ollama:
      model: "llama3:latest" # Ensure this model is pulled: `ollama pull llama3`
      base_url: "http://localhost:11434/v1"
      temperature: 0.7
      api_key: "ollama"

    # 3. Llama.cpp Server (Local - Auto-Managed)
    llama_cp_local:
      # The name of the GGUF model file in your models directory
      model: "Your-Model-Name.gguf" 
      
      # The local URL where the server will run
      base_url: "http://localhost:8080/v1"  
      
      temperature: 0.7
      api_key: "EMPTY"
      
      # [AUTO-MANAGEMENT]
      # Path to the llama-server.exe executable
      executable_path: "C:\\Path\\To\\llama-server.exe"
      
      # Directory containing your GGUF models
      models_dir: "C:\\Path\\To\\MODELS"
      
      # [OPTIONAL PARAMETERS]
      n_gpu_layers: 35    # Number of layers to offload to GPU
      # ctx_size: 32768     # Context size in tokens
      # parallel: 4         # Number of parallel requests

    # 4. vLLM (Local/Remote)
    vllm:
      model: "Qwen/Qwen2.5-1.5B-Instruct"
      base_url: "http://localhost:8000/v1"
      temperature: 0.7
      api_key: "EMPTY"

    # 5. Llama.cpp (External/Manual)
    llama_cp:
      model: "local-model-name"
      base_url: "http://localhost:1234/v1"
      temperature: 0.7
      api_key: "lm-studio"

    # 6. Llama.cpp Tunnel (Remote GPU)
    llama_cp_tunnel:
      model: "default-model"
      base_url: "http://localhost:11003/v1" # Tunnel to remote GPU (e.g., via SSH)
      temperature: 0.7
      api_key: "EMPTY"

  image_profiles:
    local_standard:
      comfy_server: "comfyui"
    remote_dgspark:
      comfy_server: "comfyui-dgspark"

# =============================================================================
# PROJECT SETTINGS
# =============================================================================
project:
  root: "outputs"
  paths:
    rag_db: "rag_db"
    characters: "Characters"
    logs: "logs"

# =============================================================================
# BOOK CONFIGURATION
# =============================================================================
book:
  title: "My New Book Title"
  genre: "Fantasy"
  theme: "Hero's Journey"
  action: "generate_book" # Options: generate_book, regenerate_pdf
  mode: "create" # Options: create, modify
  regen_images: false
  regen_chapters: []

story:
  structure:
    chapters: 2         # Short run to verify flow but enough to see progression
    scenes_per_chapter: 2
    images_per_chapter: 1
  content:
    tone: "Adventurous"
    complexity: "Medium"
    language: "English"
    word_count: 800

# =============================================================================
# CHARACTER CONFIGURATION
# =============================================================================
characters:
  generation:
    auto_create: true
    count: 2  # 1 Predefined + 2 Auto-generated = 3 total
  portraits:
    enabled: true
    workflow: "image_perfectDeliberate_text_to_image_API.json"
  predefined:
    main: 
      - "Hero Name (The Role)" # Predefined protagonist
    supporting: []

# =============================================================================
# STYLE CONFIGURATION
# =============================================================================
styles:
  pdf:
    font: "Times-Roman"
    font_size: 12
    margins: [72, 72, 72, 72] # 1 inch margins
